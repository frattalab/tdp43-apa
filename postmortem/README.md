# Liu et al. FACS-seq and NYGC RNA-seq analysis

## Prerequesites

- If its your first time running R code from this project, run `renv::restore()` in your R console to install the required dependencies
- Python/command line scripts can be executed using the general `pybioinfo` conda environment. Please see base repository README for installation instructions

Running the `tx_to_polya_quant.R` script for the FACS-seq analysis requires a custom conda environment based on the R dependencies of the PAPA pipeline. This can be installed and activated using the following commands:

```bash
<conda/mamba> env create -f papa_r.yaml # chose one of conda or mamba
conda activate papa_pipeline_r
```

## FACS-seq analysis

Broadly consists of (1) constructing the decoy transcript models, (2) quantifying with Salmon (done externally) and (3) computing PAS usage estimates and performing visualisation.

### Constructing decoy transcripts for intronic ALE and IPA events

As the FACS-seq is generated from nuclei, the RNA population comprises a mix of nascent and mature RNA. If just provide the intervals corresponding to ALEs/IPA regions, then any intronic coverage in that window will be assigned to the ALE, even if there is widespread intron retention in the same loci. By adding the alternative processing decisions into the target transcriptome, we provide an alternative origin for these reads and should limit false positive read assignment to cryptic APAs.

The workflow broadly proceeds as follows:

1. Split the PAPA quantification GTF by cryptic status (gene & event level) and event type
2. Construct decoy transcript models for intronic ALE and IPA events
3. Re-combine split PAPA quantification GTF with the ALE/IPA + decoys GTFs
4. Double check that all transcripts have been correctly assigned to event IDs as with original PAPA GTF (i.e. no differences in events we're quantifying)

### Split GTF by cryptic status and event type

`scripts/split_gtf_by_cryptic_and_event_status.py` generates GTFs for each cryptic event type as well as all non-cryptic events. GTFs for non-cryptic APAs of cryptic-containing genes are also generated for ALE and IPA events. Takes as input:

- Cryptic event summary table (produced by `../preprocessing/scripts/manual_validation_summary.R`) - `data/2023-12-10_cryptics_summary_all_events_bleedthrough_manual_validation.tsv`
- PAPA quantification GTF - `data/novel_ref_combined.quant.last_exons.gtf`


```bash
conda activate pybioinfo # if applicable
python scripts/split_gtf_by_cryptic_and_event_status.py \
  data/novel_ref_combined.quant.last_exons.gtf \
  data/2023-12-10_cryptics_summary_all_events_bleedthrough_manual_validation.tsv \
  processed/decoys/novel_ref_combined.quant
```

### Construct decoys for IPA and ALE events

`scripts/get_decoy_tx.py` constructs decoy transcripts for intronic ALE and IPA events. ALE decoys are the complete overlapping intron in which the ALE is contained. No decoys are generated for distal ALEs which have no overlapping intron. IPA events are matched and their 5' coordinates updated to the upstream, adjacent annotated exon they extend. The downstream exon in each annotated transcript is then retained to generate a 'spliced' decoy (i.e. the IPA is skipped/spliced out). An intron-retention decoy is generated by merging the spliced decoy with the intervening intron (i.e. the intron is retained and IPA is not used).

Detailed annotations of why decoys are not generated for specific events are reported for ALEs. Coverage for IPAs is more spotty, but IDs that lack a spliced decoy are reported but retained in the output.

Requires as input:

- GTFs containing cryptic ALEs/IPAs (generated by `scripts/split_gtf_by_cryptic_and_event_status.py`) - `processed/decoys/novel_ref_combined.quant.cryptics.<ipa/ale>.ids.gtf`
- GTF containing reference transcriptome - here I used the Gencode v40 filtered GTF used in the PAPA pipeline - `data/reference_filtered.gtf`

```bash
conda activate pybioinfo # if applicable
python scripts/get_decoy_tx.py -r data/reference_filtered.gtf -i processed/decoys/novel_ref_combined.quant.cryptics.ipa.ids.gtf -a processed/decoys/novel_ref_combined.quant.cryptics.ale.ids.gtf -o processed/decoys/2024-11-20_cryptics_plus_decoys &> processed/decoys/2024-10-20_get_decoy_tx.log.stderr.txt
```

### Merge decoys with original quant GTF and test ID assignment (for cryptic genes)

`scripts/merge_test_decoys_wrapper.sh` merges the decoy-augmented ALEs/IPA GTFs with other cryptic and non-cryptic events. The script also tests the constructed tx2le/ID assignment tables to confirm for no differences between assigned IDs for cryptic-containing genes after adding the decoys. Requirements/inputs are hardcoded in the script

```bash
conda activate pybioinfo # if applicable
bash scripts/merge_test_decoys_wrapper.sh processed/decoys/2024-11-20_decoys_novel_ref_combined.quant &> processed/decoys/2024-11-20_merge_test_decoys_wrapper.log.txt
```

### Quantification with Salmon

Used the `salmon` sub-pipeline available at [frattalab/rna_seq_single_steps](https://github.com/frattalab/rna_seq_single_steps) (commit hash: 5827c51). Please see linked repo for usage instructions

### PAS quantication

`scripts/summarise_quant.sh` summarises transcript-level TPMs to the isoform/le_id level & and computes PPAUs. Just a wrapper around `scripts/tx_to_polya_quant.R`, a copy of the PAPA script to merge Salmon results using tximport. Requires as input:

- Sample table mimicking the PAPA sample table that minimally contains sample names (sample_name), their sorted population (TDP-positive or negative in condition column) and additional metadata (e.g. patient id) - `data/liu_facs/liu_facs_papa_sample_sheet.tdppos_first.csv`
- Directory containing per-sample salmon quantifications as in previous step - mapped here to `data/liu_facs/2024-11-20_decoys/salmon_quant/`
- 'tx2le' table mapping transcript_id | le_id - produced by `scripts/merge_decoy_gtfs.py` (part of `scripts/merge_test_decoys_wrapper.sh`)
- 'le2gene' table mapping transcript_id | gene_id - produced by `scripts/merge_decoy_gtfs.py` (part of `scripts/merge_test_decoys_wrapper.sh`)

```bash
conda activate papa_pipeline_r # if applicable
bash scripts/summarise_quant.sh
```

### Sample-wise delta PPAU calculation

`scripts/ppau_liu_facs.R` computes the patient-level differences in PPAU between their TDP-positive and TDP-negative nuclei populations. This additional step is required because PAPA's summary script can only handle two conditions, so only calculates means and deltas considering all negative vs all positive

### Sample-wise delta PPAU visualisation

`plot_liu_facs.R` generates the heatmap of cryptic APAs with consistently enriched usage in TDPnegative populations across patients (Fig. 2A). Also outputs TSVs of summary statistics and whether events pass/miss the enrichment threshold

### PAPA to splice junctions


Commands used:

`python scripts/last_exons_to_sj.py data/2023-09-06_papa_cryptic_spliced.last_exons.cryptic_only.blacklist_filtered.bed data/reference_filtered.gtf processed/2023-09-12_papa_as_ale_cryptic &> processed/2023-09-12_papa_as_ale_cryptic.last_exons_to_sj.log.txt`

Notes: 

- 1 missing event - `ENSG00000002746.15_3|HECW1|spliced|cryptic` - which is better described as a novel IPA event that an ALE, but has been categorised as annotated by this script in this case 
  - 5'end is shifted 1 nucleotide upstream relative to annotated internal exon
  - No annotated intron/SJ that contains this exon
  - Probably initially classified as spliced because of novel exon 5'end (+ 3'end being contained within intron), but 5'end of last intron matches a known intron

### BED file of SJs from Seddighi counts table

Count table AL provided was missign ~ 200 samples (they quantified subset of NYGC), so need to re-run.
Luckily the SJ coordinates are first 6 columns in table, although want to replace Name field with 'Symbol' from df
Provided file is a CSV file, but some fields of the 'disease_full' column contain commas, which breaks a simple awk parsing of CSV files (but readr can handle)

```{r}
library(tidyverse)
df <- read_csv("data/nygc/seddighi_cryptics_nygc.csv")
write_tsv(df, "data/nygc/seddighi_cryptics_nygc.tsv")
``` 

Then ran this one-liner to extract fields of interest then drop duplicate rows

`awk -F"\t" 'BEGIN {OFS=FS} {if (NR > 1) {print $1,$2,$3,$21,".",$6}}' data/nygc/seddighi_cryptics_nygc.tsv | sort -u -k1,1 -k2,2n -k3,3n -k6,6 > data/nygc/seddighi_cryptics_sjs.bed`
