### Generating decoy index

Commands used:

`python scripts/add_decoys_to_gtf.py -i data/PAPA/2023-06-20_cryptic_last_exons.gtf -r data/reference_filtered.gtf -d full -o processed/2023-06-21_cryptics_plus_decoys.decoys_full`

`python scripts/add_decoys_to_gtf.py -i processed/2023-06-20_cryptic_last_exons.gtf -r data/reference_filtered.gtf -d full -o processed/2023-06-22_cryptics_plus_decoys.decoys_full_fix_tx2le`

add_decoys_to_gtf requirements:

- `event_type_simple` attribute, which is generated using `preproccesing/scripts/clean_papa_tbls.R`:

  - spliced - last exon with novel SJ
  - bleedthrough - last exon that extends an annotated internal exon (or annotated last exon that bleeds through )
  - distal_3utr_extension - xx

- input last exons contain a 'ref_gene_id` attribute, which corresponds to the reference gene_id to which the last exon overlaps/belongs. This is satisfied if you use 'novel_ref_combined.last_exons.gtf' generated by PAPA pipeline.

Splitting quantification GTF by cryptic status and cryptics by event type:

```bash
python scripts/split_gtf_by_cryptic_and_event_status.py data/novel_ref_combined.quant.last_exons.gtf data/2023-12-10_cryptics_summary_all_events_bleedthrough_manual_validation.tsv processed/decoys/novel_ref_combined.quant
```

#### Testing

Want to ensure that the assignment of transcripts (the input regions) to cryptic le_ids is identical between the PAPA script and `add_decoys_to_gtf.py`. To do this, first need to subset the overall tx2le table for cryptic le_ids and cryptic-containing genes. I did this lazily at the interactive session like below: 

```bash
$ pwd
<path/to/repo>/tdp43-apa/postmortem/data
$ python
```

```python
import pandas as pd
tx2le = pd.read_csv("PAPA/novel_ref_combined.tx2le.tsv", sep="\t")
cryp_summ = pd.read_csv("2023-12-10_cryptics_summary_all_events_bleedthrough_manual_validation.tsv", sep="\t")
cryp_le_ids = set(cryp_summ.le_id)
cryp_tx2le = tx2le[tx2le.le_id.isin(cryp_le_ids)]
cryp_tx2le.to_csv("novel_ref_combined.cryptic_ids.tx2le.tsv", sep="\t", index=False, header=True)
# Create one subsetting to genes (i.e. are all le_ids of cryptic genes identical?)
# ENSG00000002746.15_3 -> ENSG00000002746.15s
cryp_le_genes = {le_id.split("_")[0] for le_id in cryp_le_ids} 
tx2le.loc[:, "gene_id"] = tx2le.le_id.str.split("_", expand=True)[0]
cryp_gene_tx2le = tx2le[tx2le.gene_id.isin(cryp_le_genes)].drop("gene_id")
cryp_gene_tx2le.to_csv("novel_ref_combined.cryptic_genes.tx2le.tsv", sep="\t", index=False, header=True)
```

Next, run `scripts/test_decoys_tx_assignment.py` to check for mismatches

```bash
python scripts/test_decoys_tx_assignment.py <ORIG_CRYPTICS_TX2LE> <NEW_CRYPTICS_TX2LE>
```

```bash
python scripts/test_decoys_tx_assignment.py --output_prefix processed/2024-09-30_cryptics_decoys.cryptic_ids --metadata_file data/2023-12-10_cryptics_summary_all_events_bleedthrough_manual_validation.tsv data/novel_ref_combined.cryptic_ids.tx2le.tsv processed/2023-09-26_cryptics_plus_decoys.full.tx2le.tsv
```

Wrapper script to generate decoys then check tx assignment

```bash
bash scripts/decoys_wrapper.sh processed/decoys/2024-10-02_cryptics_plus_decoys.full
```

## Decoys take 2

### Split GTF by cryptic status and event type

```bash
python scripts/split_gtf_by_cryptic_and_event_status.py data/novel_ref_combined.quant.last_exons.gtf data/2023-12-10_cryptics_summary_all_events_bleedthrough_manual_validation.tsv processed/decoys/novel_ref_combined.quant
```

### Construct decoys for IPA and ALE events

Replacing 2024-11-06 with whatever is today's date (TODO: update when have finalised offering)

```bash
mkdir -p processed/decoys/playground/
#python scripts/get_decoy_tx.py -r data/reference_filtered.gtf -i processed/decoys/novel_ref_combined.quant.cryptics.ipa.ids.gtf -a processed/decoys/novel_ref_combined.quant.cryptics.ale.ids.gtf -o processed/decoys/playground/2024-11-06_decoys
python scripts/get_decoy_tx.py -r data/reference_filtered.gtf -i processed/decoys/novel_ref_combined.quant.cryptics.ipa.ids.gtf -a processed/decoys/novel_ref_combined.quant.cryptics.ale.ids.gtf -o processed/decoys/2024-11-20_cryptics_plus_decoys &> processed/decoys/2024-10-20_get_decoy_tx.log.stderr.txt
```

### Merge decoys with original quant GTF and test ID assignment (for cryptic genes)

Wrapper script - WIP for final commands

```bash
# bash scripts/merge_test_decoys_wrapper.sh tmp_merge_test_decoys
bash scripts/merge_test_decoys_wrapper.sh processed/decoys/2024-11-20_decoys_novel_ref_combined.quant &> processed/decoys/2024-11-20_merge_test_decoys_wrapper.log.txt
```

### PAPA to splice junctions


Commands used:

`python scripts/last_exons_to_sj.py data/2023-09-06_papa_cryptic_spliced.last_exons.cryptic_only.blacklist_filtered.bed data/reference_filtered.gtf processed/2023-09-12_papa_as_ale_cryptic &> processed/2023-09-12_papa_as_ale_cryptic.last_exons_to_sj.log.txt`

1 missing event - `ENSG00000002746.15_3|HECW1|spliced|cryptic` - which is better described as a novel bleedthrough event that an AS-ALE, but has been categorised as annotated by this scriptin this case. Why classified as a spliced event?

- 5'end is shifted 1 nucleotide upstream relative to annotated internal exon
- No annotated intron/SJ that contains this exon
- Probably initially classified as spliced because of novel exon 5'end (+ 3'end being contained within intron), but 5'end of last intron matches a known intron

### PAS quantication

- Single steps 'salmon' pipeline used to quantify decoy-augmented transcriptome. Output at `<main_output_dir>/salmon_quant` is used here

- transcript-level TPMs summarised to the isoform/le_id level & PPAUs are using the `scripts/summarise_quant.sh` script, which is just a wrapper around `scripts/tx_to_polya_quant.R`, a copy of the PAPA script to merge Salmon results using tximport

```bash
Rscript --vanilla scripts/tx_to_polya_quant.R \
-s $sample_tbl \
-d $salmon_dir \
-t $tx2le \
-g $le2gene \
-o $out_prefix &> ${out_prefix}.tx_to_polya_quant.log
```

where:

- `sample_tbl` - 'dummy' PAPA compatible sample table containing sample names and their sorted population (TDP-positive or negative) and additional metadata
- `salmon_dir` - directory to storing quantification results from Salmon single steps Snakemake pipeline
- `tx2le` - TSV mapping transcript ids to last exon ids, produced by `scripts/merge_test_decoys_wrapper.sh`
- `le2gene` - TSV mapping **transcript ids** to **gene ids**, produced by `scripts/merge_test_decoys_wrapper.sh`

### Sample-wise delta PPAU calculation

- PAPA's summary script can only handle two conditions, so only calculates means and deltas considering all negative vs all positive. 
- use `scripts/ppau_liu_facs.R` to get sample level differences

### BED file of SJs from Seddighi counts table

Count table AL provided was missign ~ 200 samples (they quantified subset of NYGC), so need to re-run.
Luckily the SJ coordinates are first 6 columns in table, although want to replace Name field with 'Symbol' from df
Provided file is a CSV file, but some fields of the 'disease_full' column contain commas, which breaks a simple awk parsing of CSV files (but readr can handle)

```{r}
library(tidyverse)
df <- read_csv("data/nygc/seddighi_cryptics_nygc.csv")
write_tsv(df, "data/nygc/seddighi_cryptics_nygc.tsv")
``` 

Then ran this one-liner to extract fields of interest then drop duplicate rows

`awk -F"\t" 'BEGIN {OFS=FS} {if (NR > 1) {print $1,$2,$3,$21,".",$6}}' data/nygc/seddighi_cryptics_nygc.tsv | sort -u -k1,1 -k2,2n -k3,3n -k6,6 > data/nygc/seddighi_cryptics_sjs.bed`
